# -*- coding: utf-8 -*-
"""MLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18myQEUjoUnvo2gSzvAP_wJs-eSsCh0vq
"""

import os
import numpy as np
from PIL import Image, UnidentifiedImageError
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.metrics import classification_report
import joblib

# Step 1: Mount Google Drive to access the images
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Preprocess the images and load labels from Google Drive
# Update the path based on your My Drive structure
images_path = '/content/drive/MyDrive/CS306_2024/COMPSYS 306/images/'

# Define the class names
class_names = ['stop', '55_speed', 'green_light', 'red_light', 'sheep']

# Lists to store images and labels
images = []
labels = []

# Load and preprocess images
for label in class_names:  # Use the folder names as labels
    folder_path = os.path.join(images_path, label)
    if os.path.isdir(folder_path):
        print(f"Processing folder: {folder_path}")
        for image_file in os.listdir(folder_path):
            image_path = os.path.join(folder_path, image_file)

            # Skip Google Drive shortcuts (these paths contain '.shortcut-targets-by-id')
            if '/.shortcut-targets-by-id/' in image_path:
                print(f"Skipping shortcut: {image_file}")
                continue

            # Skip non-image files based on their extension
            if not image_file.lower().endswith(('.png', '.jpg', '.jpeg')):
                print(f"Skipping non-image file: {image_file}")
                continue

            try:
                image = Image.open(image_path).resize((200, 200))
                image = np.array(image) / 255.0  # Normalize pixel values to [0, 1]
                images.append(image)
                labels.append(class_names.index(label))  # Use the index of the class name as the label
            except UnidentifiedImageError:
                print(f"Skipping invalid image file: {image_file}")
            except Exception as e:
                print(f"Error processing file {image_file}: {e}")

# Convert lists to NumPy arrays
images = np.array(images)
labels = np.array(labels)

# Step 3: Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)

# Step 4: Define the MLP model
model = tf.keras.Sequential([
    tf.keras.Input(shape=(200, 200, 3)),  # Input shape matches the image dimensions (32x32 with 3 color channels)
    tf.keras.layers.Flatten(),  # Flatten the input image
    tf.keras.layers.Dense(512, activation='relu'),  # Fully connected layer with 512 units
    tf.keras.layers.Dense(256, activation='relu'),  # Fully connected layer with 256 units
    tf.keras.layers.Dense(256, activation='relu'),  # Fully connected layer with 256 units
    tf.keras.layers.Dense(64, activation='relu'),   # Fully connected layer with 64 units
    tf.keras.layers.Dense(len(class_names), activation='softmax')  # Output layer with 5 classes
])

# Step 5: Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

# Step 6: Add EarlyStopping callback to optimize the number of epochs
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Step 7: Train the model with EarlyStopping
history = model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])

# Step 8: Evaluate the model on the test set
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print('\nTest accuracy:', test_acc)